from transformers import BertTokenizer, BertForSequenceClassification
import asyncio
from django.core.management.base import BaseCommand, CommandError
import time
from icecream import ic
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import asyncio

class ToxicityAnalyzer:
    tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier').to(device) # type: ignore 
        
    def __init__(self) -> None:
        pass

    def text_to_prob(self, text:str):
        # prepare the input
        batch = self.tokenizer.encode(text, return_tensors='pt').to(self.device) # type: ignore # type: ignore # type: ignore # type: ignore # type: ignore

        # inference
        with torch.no_grad():
            outputs = self.model(batch) # type: ignore
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # interpreting the output
        toxicity_prob = round(probs[0][1].item(), 3)
        neutral_prob = round(probs[0][0].item(), 3)

        res = {'neutral': neutral_prob, 'toxic': toxicity_prob}
        #breakpoint()
        return res

def show(neural_network, texts:list[str]):
    start = time.perf_counter()
    ic()
    ic(neural_network.text_to_prob('–î–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö —ç–º–æ—Ü–∏–π'))
    ic(neural_network.text_to_prob('–¢—ã –º–Ω–µ –Ω—Ä–∞–≤–∏—à—å—Å—è. –Ø —Ç–µ–±—è –ª—é–±–ª—é'))
    ic(neural_network.text_to_prob('–ú–∏–Ω–∞—Ç–æ, —Ç—ã –ø–µ—Ä–µ—Å—Ç—É–ø–∞–µ—à—å —á–µ—Ä—Ç—É...'))
    ic(neural_network.text_to_prob('–ü—Ö–µ—Ö –ø—Ä–∏–º–∞–Ω–æ—á–∫–∞ XD'))
    ic(neural_network.text_to_prob('–°–ø–∞—Å–∏–±–æ.'))
    ic(neural_network.text_to_prob('–ù–µ –ø—Ñ—ã–∫–∞–π –º–Ω–µ —Ç—É—Ç! üò†'))
    ic(neural_network.text_to_prob('–≠—Ç–æ—Ç –ø—Ä–∏—Ü–µ–ª –ø—Ä–æ—Å—Ç–æ –∏–º–±–∞'))
    ic(neural_network.text_to_prob('–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –º–∏—Ä –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º—É–∂—á–∏–Ω'))
    ic(neural_network.text_to_prob('–ê—Ö–∞—Ö—ç–∞—Ö–∞—Ö<br>–≠—Ç–æ –∂–µ –≥–µ–Ω–∏–∞–ª—å–Ω–æ, –±–µ—Ä—ë–º —É—Ä–æ–∫–∏ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –æ—Ç –î–∂–∏–Ω–Ω–∏!'))
    ic()
    end = time.perf_counter()
    ic(end-start)

class EmotionAnalyzer:
    tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2-cedr-emotion-detection')
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    model = BertForSequenceClassification.from_pretrained('cointegrated/rubert-tiny2-cedr-emotion-detection').to(device) # type: ignore 
    #if torch.cuda.is_available():
    #    model.to('cuda')
        
    def __init__(self) -> None:
        pass
    
    def text_to_prob(self, text:str):
        # prepare the input
        batch = self.tokenizer.encode(text, return_tensors='pt').to(self.device) # type: ignore # type: ignore # type: ignore # type: ignore # type: ignore

        # inference
        with torch.no_grad():
            outputs = self.model(batch) # type: ignore
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # interpreting the output
        no_emotion_prob = round(probs[0][0].item(), 3)
        sadness_prob = round(probs[0][1].item(), 3)
        surprise_prob = round(probs[0][2].item(), 3)
        anger_prob = round(probs[0][3].item(), 3)
        fear_prob = round(probs[0][4].item(), 3)
        joy_prob = round(probs[0][5].item(), 3)
        
        res = {'no_emotion':no_emotion_prob, 'sadness':sadness_prob, 'surprise':surprise_prob, 'anger':anger_prob, 'fear':fear_prob, 'joy':joy_prob}
        
        return res

class SmallToxicityAnalyzer:
    tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny-toxicity') # 11.8M 
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    device = 'cuda'
    model = BertForSequenceClassification.from_pretrained('cointegrated/rubert-tiny-toxicity').to(device)  # type: ignore 
    #if torch.cuda.is_available():
    #    model.to('cuda')
        
    def __init__(self) -> None:
        pass

    async def text_to_prob(self, text:str):
        
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(self.device) # type: ignore 
            proba = torch.sigmoid(self.model(**inputs).logits).cpu().numpy()[0]

        res = {'toxic': round(1 - proba.T[0] * (1 - proba.T[-1]), 3)}
        #breakpoint()
        return res

class SmallEmotionAnalyzer:
    # seara/rubert-tiny2-ru-go-emotions
    tokenizer = AutoTokenizer.from_pretrained('seara/rubert-tiny2-ru-go-emotions')
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    model = AutoModelForSequenceClassification.from_pretrained('seara/rubert-tiny2-ru-go-emotions')
    
    def __init__(self) -> None:
        self.types = {
            'admiration': '–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ',
            'amusement': '–≤–µ—Å–µ–ª—å–µ',
            'anger': '–∑–ª–æ—Å—Ç—å',
            'annoyance': '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ',
            'approval': '–æ–¥–æ–±—Ä–µ–Ω–∏–µ',
            'caring': '–∑–∞–±–æ—Ç–∞',
            'confusion': '–Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ',
            'curiosity': '–ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ',
            'desire': '–∂–µ–ª–∞–Ω–∏–µ',
            'disappointment': '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ',
            'disapproval': '–Ω–µ–æ–¥–æ–±—Ä–µ–Ω–∏–µ',
            'disgust': '–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ',
            'embarrassment': '—Å–º—É—â–µ–Ω–∏–µ',
            'excitement': '–≤–æ–∑–±—É–∂–¥–µ–Ω–∏–µ',
            'fear': '—Å—Ç—Ä–∞—Ö',
            'gratitude': '–ø—Ä–∏–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
            'grief': '–≥–æ—Ä–µ',
            'joy': '—Ä–∞–¥–æ—Å—Ç—å',
            'love': '–ª—é–±–æ–≤—å',
            'nervousness': '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å',
            'optimism': '–æ–ø—Ç–∏–º–∏–∑–º',
            'pride': '–≥–æ—Ä–¥–æ—Å—Ç—å',
            'realization': '–æ—Å–æ–∑–Ω–∞–Ω–∏–µ',
            'relief': '–æ–±–ª–µ–≥—á–µ–Ω–∏–µ',
            'remorse': '—Ä–∞—Å–∫–∞—è–Ω–∏–µ',
            'sadness': '–≥—Ä—É—Å—Ç—å',
            'surprise': '—É–¥–∏–≤–ª–µ–Ω–∏–µ',
            'neutral': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å',
        }

    def text_to_prob(self, text:str):
        # prepare the input
        batch = self.tokenizer.encode(text, return_tensors='pt') # type: ignore # type: ignore # type: ignore # type: ignore # type: ignore

        # inference
        with torch.no_grad():
            outputs = self.model(batch) # type: ignore
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]
        
        res = {}
        
        for t, prob in zip(self.types.values(), probs):
            res[t] = round(prob.item(), 3)
        
        return res

class EmotionAnalizerNew:
    LABELS = ['neutral', 'happiness', 'sadness', 'enthusiasm', 'fear', 'anger', 'disgust']
    tokenizer = AutoTokenizer.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection') # 29.2M 
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    device = 'cuda'
    model = BertForSequenceClassification.from_pretrained('Aniemore/rubert-tiny2-russian-emotion-detection').to(device)  # type: ignore 
        
    def __init__(self) -> None:
        pass

    async def text_to_prob(self, text:str):
        
        with torch.no_grad():
            inputs = self.tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(self.device)
            outputs = self.model(**inputs)
            
        predicted = torch.nn.functional.softmax(outputs.logits, dim=1)
        max_index = predicted.argmax().item()
        max_emotion = self.LABELS[max_index] # type: ignore 
        max_prob = predicted[0, max_index].item()  # type: ignore 
        #breakpoint()
        return {max_emotion: round(max_prob, 3)}


class Processor:
    def __init__(self) -> None:
        self.t = SmallToxicityAnalyzer()
        self.e = EmotionAnalizerNew()

    async def text_to_res(self, text:str):
        res = {}
        res_e = asyncio.create_task(self.e.text_to_prob(text))
        res_t = asyncio.create_task(self.t.text_to_prob(text))
        res.update(await res_e)
        res.update(await res_t)
        return res
        
class Command(BaseCommand):
    help = 'Closes the specified poll for voting'
    
    def handle(self, *args, **options):
        texts = [
            '–î–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö —ç–º–æ—Ü–∏–π', 
            '–¢—ã –º–Ω–µ –Ω—Ä–∞–≤–∏—à—å—Å—è. –Ø —Ç–µ–±—è –ª—é–±–ª—é', 
            '–ú–∏–Ω–∞—Ç–æ, —Ç—ã –ø–µ—Ä–µ—Å—Ç—É–ø–∞–µ—à—å —á–µ—Ä—Ç—É...', 
            '–ü—Ö–µ—Ö –ø—Ä–∏–º–∞–Ω–æ—á–∫–∞ XD', 
            '–°–ø–∞—Å–∏–±–æ.', 
            '–ù–µ –ø—Ñ—ã–∫–∞–π –º–Ω–µ —Ç—É—Ç! üò†',
            '–≠—Ç–æ—Ç –ø—Ä–∏—Ü–µ–ª –ø—Ä–æ—Å—Ç–æ –∏–º–±–∞',
            '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –º–∏—Ä –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º—É–∂—á–∏–Ω'
            '–ê—Ö–∞—Ö—ç–∞—Ö–∞—Ö<br>–≠—Ç–æ –∂–µ –≥–µ–Ω–∏–∞–ª—å–Ω–æ, –±–µ—Ä—ë–º —É—Ä–æ–∫–∏ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –æ—Ç –î–∂–∏–Ω–Ω–∏!'
            ] * 100
        p = Processor()
        start = time.perf_counter()
        for text in texts:
            print()
            ic(text, asyncio.run(p.text_to_res(text)))
            print()
        end = time.perf_counter()
        ic(end-start)
        ic((end-start)/len(texts)*100)
        breakpoint()
        
        
        
        
        





        